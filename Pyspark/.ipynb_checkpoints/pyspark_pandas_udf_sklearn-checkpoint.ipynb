{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Масштабируемое прогнозирование с использованием scikit-learn и PySpark Pandas UDFs -   User-Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Одна из проблем заключается в том, что передача данных между а) процессами Spark Scala на основе Scala, которые отправляют данные между машинами и могут эффективно выполнять преобразования, и б) процесс Python (например, для прогнозирования с помощью scikit-learn) несет некоторые накладные расходы из-за сериализации и межпроцессной коммуникации. Одним из решений для этого является пользовательские функции (UDF) в API DataFrame от PySpark. Вы можете использовать API DataFrame для эффективного выполнения большинства операций на Scala (без необходимости писать на Scala!), Но затем вызывать Python UDF, которые несут накладные расходы на Scala-Python только тогда, когда это необходимо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Обычные UDF PySpark работают с одним значением каждый момент времени, что приводит к большому объему накладных расходов на Scala-Python. Недавно PySpark добавил Pandas UDFs, которые эффективно конвертируют куски столбцов DataFrame в объекты Pandas Series, через Apache Arrow, чтобы избежать значительных иoverhead обычных UDF. Имея специальные UDF, Pandas Series сохраняют преобразование между представлениями с плавающей запятой Python и NumPy для scikit-learn, как это было бы необходимо для обычного UDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Начальные установки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.6/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.6/site-packages (from pyarrow) (1.14.5)\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from pyarrow) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Генерирование синтетических данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make some fake data and train a model.\n",
    "n_samples_test = 100000\n",
    "n_samples_train = 1000\n",
    "n_samples_all = n_samples_train + n_samples_test\n",
    "n_features = 50\n",
    "\n",
    "X, y = make_classification(n_samples=n_samples_all, n_features=n_features, random_state=123)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=n_samples_test, random_state=45)\n",
    "\n",
    "# Use pandas to put the test data in parquet format to illustrate how to load it up later.\n",
    "# In real usage, the data might be on S3, Azure Blog Storage, HDFS, etc.\n",
    "column_names = [f'feature{i}' for i in range(n_features)]\n",
    "(\n",
    "    pd.DataFrame(X_test, columns=column_names)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'id'})\n",
    "    .to_parquet('unlabeled_data')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Обучение модели на  scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.959\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators': [100], 'max_depth': [2, 4, None]}\n",
    "gs_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc'\n",
    ").fit(X_train, y_train)\n",
    "print('ROC AUC: %.3f' % gs_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Создание Spark драйвера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"foo\")\n",
    "sqlContext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Загрузим данные и посмотрим как они выглядят в формате parquet.\n",
    "\n",
    "В реальном применении обычно приходится делать целый набор ETL процедур после чтения сырых данных, но здесь  это просто загрузка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, feature0: double, feature1: double, feature2: double, feature3: double, feature4: double, feature5: double, feature6: double, feature7: double, feature8: double, feature9: double, feature10: double, feature11: double, feature12: double, feature13: double, feature14: double, feature15: double, feature16: double, feature17: double, feature18: double, feature19: double, feature20: double, feature21: double, feature22: double, feature23: double, feature24: double, feature25: double, feature26: double, feature27: double, feature28: double, feature29: double, feature30: double, feature31: double, feature32: double, feature33: double, feature34: double, feature35: double, feature36: double, feature37: double, feature38: double, feature39: double, feature40: double, feature41: double, feature42: double, feature43: double, feature44: double, feature45: double, feature46: double, feature47: double, feature48: double, feature49: double, __index_level_0__: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled = sqlContext.read.parquet('unlabeled_data')\n",
    "df_unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Вычисление предикций с использование regular UDF -\n",
    "\n",
    "Сначала попробуем regular UDF.  При этом будет десериализована одна строка (например, instance, sample, record) в каждый момент времени, сделано прогнозирование, и  prediction будет возвращена, затем будет сериализована послана  обратно в Spark, чтобы скомбинироваться с со всеми другими predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@F.udf(returnType=DoubleType())\n",
    "def predict_udf(*cols):\n",
    "    # cols will be a tuple of floats here.\n",
    "    return float(gs_rf.predict_proba((cols,))[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 132 ms, sys: 16 ms, total: 148 ms\n",
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_pred_a = df_unlabeled.select(\n",
    "    F.col('id'),\n",
    "    predict_udf(*column_names).alias('prediction')\n",
    ")\n",
    "df_pred_a.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Вычисление предикций с использованием Pandas UDF\n",
    "\n",
    "Теперь используем Pandas UDF (например, vectorized UDF).  В этом случае Spark будет посылать кортеж (tuple) из pandas Series objects с многими строками в один момент времени.  Кортеж будет иметь одну  Series per column/feature, для того чтобы они могли быть переданы UDF.  Обратите внимание, что один из этих Series objects не будут содержать features для всех строк сразу, потому что Spark распределяет  datasets по многим workers.  Мы будем здесь использовать partition size по умолчанию, но эта величина может быть настроена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@F.pandas_udf(returnType=DoubleType())\n",
    "def predict_pandas_udf(*cols):\n",
    "    # cols will be a tuple of pandas.Series here.\n",
    "    X = pd.concat(cols, axis=1)\n",
    "    return pd.Series(gs_rf.predict_proba(X)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 12 ms, total: 136 ms\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_pred_b = df_unlabeled.select(\n",
    "    F.col('id'),\n",
    "    predict_pandas_udf(*column_names).alias('prediction')\n",
    ")\n",
    "df_pred_b.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Вычисление  multiclass predictions\n",
    "\n",
    "Выше мы просто возвращали одну серию предсказаний для положительного класса, который работает для одной бинарной или зависимых переменных. В Pandas UDF можно также разместить многоклассовые или многосегментные модели. При этом будет возвращен набор списков чисел вместо одного ряда чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@F.pandas_udf(returnType=ArrayType(DoubleType()))\n",
    "def predict_pandas_udf(*cols):\n",
    "    X = pd.concat(cols, axis=1)\n",
    "    return pd.Series(row.tolist() for row in gs_rf.predict_proba(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 16 ms, total: 132 ms\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_pred_multi = (\n",
    "    df_unlabeled.select(\n",
    "        F.col('id'),\n",
    "        predict_pandas_udf(*column_names).alias('predictions')\n",
    "    )\n",
    "    # Select each item of the prediction array into its own column.\n",
    "    .select(\n",
    "        F.col('id'),\n",
    "        *[F.col('predictions')[i].alias(f'prediction_{c}')\n",
    "          for i, c in enumerate(gs_rf.classes_)]\n",
    "    )\n",
    ")\n",
    "df_pred_multi.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (5.4.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=33569599488, available=14229962752, percent=57.6, used=18906247168, free=11311476736, active=19825520640, inactive=1309306880, buffers=1173848064, cached=2178027520, shared=16244736, slab=880906240)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(0,10)).count()\n",
    "sc.parallelize(range(0,20)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of Slides"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
